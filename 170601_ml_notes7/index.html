<!DOCTYPE html>
<html lang="zh-cn">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>Machine Learning Notes 07 | Jellyland</title>
<meta name="title" content="Machine Learning Notes 07" />
<meta name="description" content="" />
<meta name="keywords" content="Machine Learning," />


<meta property="og:url" content="https://jrd.pub/170601_ml_notes7/">
  <meta property="og:site_name" content="Jellyland">
  <meta property="og:title" content="Machine Learning Notes 07">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:published_time" content="2017-06-01T00:00:00+00:00">
    <meta property="article:modified_time" content="2017-06-01T00:00:00+00:00">
    <meta property="article:tag" content="Machine Learning">




  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Machine Learning Notes 07">




  <meta itemprop="name" content="Machine Learning Notes 07">
  <meta itemprop="datePublished" content="2017-06-01T00:00:00+00:00">
  <meta itemprop="dateModified" content="2017-06-01T00:00:00+00:00">
  <meta itemprop="wordCount" content="544">
  <meta itemprop="keywords" content="Machine Learning">
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
  :root {
    --width: 720px;
    --font-main: Verdana, sans-serif;
    --font-secondary: Verdana, sans-serif;
    --font-scale: 1em;
    --background-color: #fff;
    --heading-color: #222;
    --text-color: #444;
    --link-color: #3273dc;
    --visited-color: #8b6fcb;
    --code-background-color: #282a36;
    --code-color: #222;
    --blockquote-color: #222;
  }

   

  body {
    font-family: var(--font-secondary);
    font-size: var(--font-scale);
    margin: auto;
    padding: 20px;
    max-width: var(--width);
    text-align: left;
    background-color: var(--background-color);
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: var(--text-color);
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    font-family: var(--font-main);
    color: var(--heading-color);
  }

  a {
    color: var(--link-color);
    cursor: pointer;
    text-decoration: none;
  }

  a:hover {
    text-decoration: underline;
  }

  nav a {
    margin-right: 8px;
  }

  strong,
  b {
    color: var(--heading-color);
  }

  button {
    margin: 0;
    cursor: pointer;
  }

  time {
    font-family: monospace;
    font-style: normal;
    font-size: 15px;
  }

  main {
    line-height: 1.6;
  }

  table {
    width: 100%;
  }

  hr {
    border: 0;
    border-top: 1px dashed;
  }

  img {
    max-width: 100%;
    display: block;
    margin: auto;
  }

  code {
    font-family: monospace;
    padding: 2px;
    background-color: var(--code-background-color);
    < !-- color: var(--code-color);
    -->border-radius: 3px;
  }

  blockquote {
     
     
     
     
  }

  footer {
    padding: 25px 0;
    text-align: center;
  }

  .title:hover {
    text-decoration: none;
  }

  .title h1 {
    font-size: 1.5em;
  }

  .inline {
    width: auto !important;
  }

  .highlight,
  .code {
    padding: 1px 15px;
    background-color: var(--code-background-color);
    color: var(--code-color);
    border-radius: 3px;
    margin-block-start: 1em;
    margin-block-end: 1em;
    overflow-x: auto;
  }

   
  ul.blog-posts {
    list-style-type: none;
    padding: unset;
  }

  ul.blog-posts li {
    display: flex;
  }

  ul.blog-posts li span {
    flex: 0 0 130px;
  }

  ul.blog-posts li a:visited {
    color: var(--visited-color);
  }
</style>
</head>

<body>
  <header><a href="/" class="title">
  <h2>Jellyland</h2>
</a>
<nav>
<a href="/now">Now</a>

<a href="/about">About</a>

<a href="/index.xml">RSS</a>

</nav>
</header>
  <main>
<div style="
    display: flex;
    justify-content: space-between;
    align-items: end;
">
  <h1>Machine Learning Notes 07</h1>
  
  <p>
    <time datetime='2017-06-01'>
      Jun 01, 2017
    </time>
  </p>
  
</div>
<content>
  <h2 id="what-should-we-do-if-we-have-a-bad-predictions">What should we do if we have a bad predictions?</h2>
<p>The following options may be feasible:</p>
<ul>
<li>Get more training examples;</li>
<li>Try smaller sets of features;</li>
<li>Try getting additional features;</li>
<li>Try adding polynomial features (increasing degree of polynomial);</li>
<li>Try decreasing \(\lambda\);</li>
<li>Try increasing \(\lambda\);</li>
</ul>
<p>So we can take a kind of test called <strong>machine learning diagnostic</strong> to insight what is/isn&rsquo;t working with a learning algorithm, and gain guidance as to how best to improve its performance. It may take time to implement, but doing so can be vary good use of your time.</p>
<hr>
<h2 id="evaluating-a-hypothesis">Evaluating a hypothesis</h2>
<p>To evaluating whether a hypothesis is good or bad (and we don&rsquo;t have extra test set), we can divide our training set at first to two parts (usually according to 7/3 proportion), and one for training, the other for testing, which helps us to avoid <strong>over fitting</strong> (perform on training set well, but bad on new examples not in training set)</p>
<hr>
<h2 id="model-selection">Model Selection</h2>
<p>Take linear regression as a example, we use \(h_{\theta}(x) = \theta_0 +\theta_1 x + \theta_2 x^2 + \cdots\) as our hypothesis function, and we can add the degree of polynomial to make our hypothe/.sis better, but it may brings the over fitting problem, so we need to find out the best degree.</p>
<p>To  achieve so, we can compute the \(J_{test}\)(cost of different degrees \(d\) on test set), and choose the best \(d\), but it&rsquo;s only fit to the test set. So we divide the data set into three parts:</p>
<ul>
<li>training set (60% usually)</li>
<li>cross validation set (20% usually)</li>
<li>test set (20% usually)(check if the combo of \(\theta\) and \(\lambda\) has a good generalization of the problem, avoiding over fitting)</li>
</ul>
<p>and three kinds of cost function is \(J_{train}\), \(J_{cv}\), \(J_{test}\).</p>
<p>Then for different \(d(d=1, 2, 3, \cdots)\), minimize \(J(\theta)\) with \(J_{train}\) and \(J_{test}\), then compute the \(J_{cv}\) for each \(d\). We choose \(d\) who has the lowest \(J_{cv}\), and that.s how we do the model selection.</p>
<hr>
<h2 id="bias-vs-variance">Bias vs. Variance</h2>
<p><img src="/article_imgs/bias_vs_var.JPG" alt=""></p>
<h3 id="diagnosing-bias-vs-variance">Diagnosing bias vs. variance</h3>
<ul>
<li>If \(J_{train}\) is high and \(J_{cv}\approx J_{train}\), we can tell it&rsquo;s a <strong>bias</strong> problem;</li>
<li>If \(J_{train}\) is low and \(J_{cv}\gg J_{train}\), we can tell it&rsquo;s a <strong>variance</strong> problem;</li>
</ul>
<hr>
<h2 id="about-regularization">About regularization</h2>
<p>As we know, appropriate \(\lambda\) (regularization parameter) can help to prevent over fitting, but when the \(\lambda\) is too large or too small. it won&rsquo;t work as so:</p>
<p><img src="/article_imgs/different_lambda.JPG" alt=""></p>
<p>So choosing appropriate value of \(\lambda\) is very necessary.</p>
<p>We can try different \(\lambda\), minimize \(J(\theta)\), then compute \(J_{cv}\) (like what we do the model selection), and we can finf the best \(\lambda\).</p>
<hr>
<h2 id="learning-curves">Learning Curves</h2>
<p>The learning curves describe the relationship of \(m\)(training set size) and error(\(J_{cv},J_{train}\)), it&rsquo;s look like:</p>
<p><img src="/article_imgs/learningcurves.JPG" alt=""></p>
<p>And if the algorithm is suffering from high bias, the learning curve is look like:</p>
<p><img src="/article_imgs/high_bias_curve.JPG" alt=""></p>
<p>We can see from the graph that the increasing \(m\) doesn&rsquo;t help to lower the bias, so we can conclude that getting more training data will not help to solve high-bias problem.</p>
<p>While if the algorithm is suffering from high variance, the learning curve is look like:</p>
<p><img src="/article_imgs/high_var_curve.JPG" alt=""></p>
<p>From the graph we can see that there is a gap between \(J_{cv}\) and \(J_{train}\), and as \(m\) increasing, the gap diminishes and the bias is also decreasing. So we can conclude that getting more training data is helpful to solve the high-variance problem.</p>
</content>
<p>
  
  <a href="https://jrd.pub/tags/machine-learning/">#Machine Learning</a>
  
</p>

<div id="gitalk-container"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script>
  var gittalk = new Gitalk({
    clientID: '',
    clientSecret: '',
    repo: 'gabrieldrapor.github.io',
    owner: 'GabrielDrapor',
    admin: ['GabrielDrapor'],
    id: location.pathname,      
    distractionFreeMode: false  
  })
  gittalk.render("gitalk-container")
</script>


  </main>

  <footer>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css"
    integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"
    integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"
    integrity="sha256-ExtbCSBuYA7kq1Pz362ibde9nnsHYPt6JxuxYeZbU+c=" crossorigin="anonymous"></script>
<script>renderMathInElement(document.body);</script>


<div class="outer">


    <blockquote>
        <p style="font-family: monospace;font-style: italic;">"It’s not magic, it’s talent and sweat."</p>
    </blockquote>
    <hr style="width:10vw;margin: 5vh auto; border: 0; border-width: 1px; border-style: inset;">

    <div id="footer-info" class="inner">
        &copy; 2025 Jellyland

        <br />
        Powered by <a href="https://gohugo.io" target="_blank">Hugo</a> with theme <a href="https://github.com/janraasch/hugo-bearblog"
            target="_blank">hugo-bearblog</a>
    </div>
</div>

</footer>
</body>

</html>