<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">

<title>
Machine Learning Notes 05 - DRAPORLAND
</title>











<link rel="stylesheet" href="/css/main.min.81bbafc4df93b11c1c3e2449464373c384aa4903731b4fc7a77dfcdd979e184f.css" integrity="sha256-gbuvxN&#43;TsRwcPiRJRkNzw4SqSQNzG0/Hp3383ZeeGE8=" crossorigin="anonymous" media="screen">



 

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Machine Learning Notes 05"/>
<meta name="twitter:description" content=" "/>

<meta property="og:title" content="Machine Learning Notes 05" />
<meta property="og:description" content=" " />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://drapor.me/posts/161202_ml_notes5/" />
<meta property="article:published_time" content="2016-12-02T00:00:00+00:00" />
<meta property="article:modified_time" content="2016-12-02T00:00:00+00:00" />



    

    
    
    
    <title>
        
        Machine Learning Notes 05
        
    </title>
    <style>
        #container .reply-content {
    	    margin-bottom: 10px;
            margin-top: 8px;
            color: #ddd;
        }
    </style>
</head>

<body>
    <div class="wrap">
        <div class="section" id="title">Machine Learning Notes 05</div>

        
<div class="section" id="content">
    Fri Dec 02, 2016
    
    <div class="tag-container">
        
        <span class="tag">
            <a href="https://drapor.me/tags/machine-learning">
                #Machine Learning
            </a>
        </span>
        
    </div>
    
    <hr/>
    <p> </p>
<h2 id="overfitting">Overfitting</h2>
<p>Overfitting would cause the modle to perform poorly, like this:
<img src="/article_imgs/reg_exmp1.PNG"></p>
<img src="/article_imgs/reg_exmp2.PNG">
<p>  As above, plotting the hypothesis could be one way to try to decide what degree polynomial to use, but that doesn't always work. Sometims we have too many features which cause that it's difficult to visulize. And if we have a lot of features and very little training data, then overfitting can become a problem.</p>
<hr>
<h2 id="two-options-to-solve">Two options to solve</h2>
<p><strong>1.Reduce the number of features</strong></p>
<blockquote>
<p>Manually select which features to keep or throw out;<br>
Model selection algorithm (help ue to decide which features to keep or throw out automatically);<br>
Disadvantages: Reduce some information about the problem.</p>
</blockquote>
<p><strong>2.Regularization</strong></p>
<blockquote>
<p>Keep all the features, but reduce magnitude/values of parameters <code>$\theta_{j}$</code>.
Works well when we have a lot of features, each of which contributes a bit to predicting $y$.</p>
</blockquote>
<p>So we can see regularization is a better choice mostly.</p>
<hr>
<h2 id="see-the-cost-functinon">See the cost functinon</h2>
<ul>
<li>First, consider this. If our hypothesis is like this:</li>
</ul>
<p><code>$$\theta_{0} + \theta_{1}x + \theta_{2}x^{2} + \theta_{3}x^3 + \theta_{4}x^4$$</code></p>
<p>and if we penalize and make <code>$\theta_{3}$</code> and <code>$\theta_{4}$</code> really small, it means that <code>$\theta_{3}\approx0,\theta_{4}\approx0$</code>, that is like as if we &lsquo;re getting rid of these two terms, then we would find that
<code>$$\theta_{0} + \theta_{1}x + \theta_{2}x^{2} + \theta_{3}x^3 + \theta_{4}x^4\approx\theta_{0} + \theta_{1}x + \theta_{2}x^{2} $$</code></p>
<hr>
<h2 id="regularization">Regularization</h2>
<p>“…having <strong>smaller values of the parameters</strong> corresponds to usually <strong>smoother functions as well for the simpler</strong>. And which are therefore, also, <strong>less prone to overfitting</strong>. ”
What we should do is to modify the cost function to shrink all of the parameters like this:
<code>$$J(\theta)=\frac{1}{2m}\sum_{i=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})^{2} + {\color{Blue} {\lambda\sum_{i=1}^{m}\theta_{j}^{2}}}$$</code></p>
<p>And the blue terms is called <strong>regularization terms</strong>, $\lambda$ is called <strong>regularization parameters</strong> which is used to trade off between two different goals:<br>
`$$\begin{cases}</p>
<ol>
<li>\sum(h_{\theta}(x^{(i)})-y^{(i)})^{2}:-:fit:well\</li>
<li>\sum{\theta_{j}^{2}}:-:keep:the:parameters:small
\end{cases}$$`</li>
</ol>
<p>By the way, we don't penalize $\theta_{0}$ by convention.
Besides, about the regularization parameters $\lambda$, if $\lambda$ is too large, it would cause underfitting, and if $\lambda$ is too small, it may cause the useless regularization.</p>
<hr>
<h2 id="regularized-linear-regression">Regularized linear regression</h2>
<ul>
<li>Gradient descent:
Old:
   Repeat{<br>
      <code>$\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}\:(j=0,1,2,3,...,n)$</code>.
   }</li>
</ul>
<p><font color=blue>New:</font><br>
   <font color=blue>Repeat{</font><br>
      <code>${\color{Blue} {\theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum_{i=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})x_{0}^{(i)}}}$</code><br>
      <code>${\color{Blue} {\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}\:(j=1,2,...,n)}}{\color{Red} {+\frac{\lambda}{m}\theta_{j}}}$</code><br>
   <font color=blue>}</font></p>
<hr>
<h2 id="regularization-logistic-regression">Regularization logistic regression</h2>
<p>   Cost function <code>$J(\theta)=-ylog(h_{\theta}(x))-(1-y)log(1-h_{\theta}(x)){\color{Blue} {+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}}}$</code></p>
<p>   <font color=blue>Repeat{</font><br>
      <code>${\color{Blue} {\theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum_{i=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})x_{0}^{(i)}}}$</code><br>
      <code>$\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}\:(j=1,2,...,n){\color{Red} {+\frac{\lambda}{m}\theta_{j}}}$</code><br>
   <font color=blue>}</font><br>
Remember that <code>$h_{\theta}(x)=\frac{1}{1+e^{-\theta^{T}x}}$</code></p>
</div>


        
<div class="section bottom-menu">
    
<hr />
<p>


    
        <a href="/posts">back</a>
        
            &#183;
        
    

    
        
            <a href="/posts">Posts</a>
        
    
    
        
            &#183; 
            <a href="/scribble">Scribbles</a>
        
            &#183; 
            <a href="https://o3o.ca/@drapor">Feed</a>
        
            &#183; 
            <a href="/about">About Me</a>
        
    
    &#183; 
    <a href="https://drapor.me">
        
    </a>

</p>
</div>



        <div class="section footer">Copyright © 2020 Gabriel Drapor, All Rights Reserved.</div>


	
     <div id="gitalk-container"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script>
  var gittalk = new Gitalk({
      clientID: 'b33898060f51f3637cdb',
      clientSecret: '264aa9f9ab82c827765c14f5df4d8195bfd26a0d',
      repo: 'gabrieldrapor.github.io',
      owner: 'GabrielDrapor',
      admin: ['GabrielDrapor'],
      id: location.pathname,      
      distractionFreeMode: false  
  })
  gittalk.render("gitalk-container")
</script>
</body>

</html>
