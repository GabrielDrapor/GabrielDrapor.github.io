<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">

<title>
Machine Learning Notes 07 - DRAPORLAND
</title>











<link rel="stylesheet" href="/css/main.min.81bbafc4df93b11c1c3e2449464373c384aa4903731b4fc7a77dfcdd979e184f.css" integrity="sha256-gbuvxN&#43;TsRwcPiRJRkNzw4SqSQNzG0/Hp3383ZeeGE8=" crossorigin="anonymous" media="screen">



 

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Machine Learning Notes 07"/>
<meta name="twitter:description" content="What should we do if we have a bad predictions?"/>

<meta property="og:title" content="Machine Learning Notes 07" />
<meta property="og:description" content="What should we do if we have a bad predictions?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://drapor.me/posts/170601_ml_notes7/" />
<meta property="article:published_time" content="2017-06-01T00:00:00+00:00" />
<meta property="article:modified_time" content="2017-06-01T00:00:00+00:00" />



    

    
    
    
    <title>
        
        Machine Learning Notes 07
        
    </title>
    <style>
        #container .reply-content {
    	    margin-bottom: 10px;
            margin-top: 8px;
            color: #ddd;
        }
    </style>
</head>

<body>
    <div class="wrap">
        <div class="section" id="title">Machine Learning Notes 07</div>

        
<div class="section" id="content">
    Thu Jun 01, 2017
    
    <div class="tag-container">
        
        <span class="tag">
            <a href="https://drapor.me/tags/machine-learning">
                #Machine Learning
            </a>
        </span>
        
    </div>
    
    <hr/>
    <h2 id="what-should-we-do-if-we-have-a-bad-predictions">What should we do if we have a bad predictions?</h2>
<p>The following options may be feasible:</p>
<ul>
<li>Get more training examples;</li>
<li>Try smaller sets of features;</li>
<li>Try getting additional features;</li>
<li>Try adding polynomial features (increasing degree of polynomial);</li>
<li>Try decreasing $\lambda$;</li>
<li>Try increasing $\lambda$;</li>
</ul>
<p>So we can take a kind of test called <strong>machine learning diagnostic</strong> to insight what is/isn&rsquo;t working with a learning algorithm, and gain guidance as to how best to improve its performance. It may take time to implement, but doing so can be vary good use of your time.</p>
<p><escape><!-- more --></escape></p>
<hr>
<h2 id="evaluating-a-hypothesis">Evaluating a hypothesis</h2>
<p>To evaluating whether a hypothesis is good or bad (and we don&rsquo;t have extra test set), we can divide our training set at first to two parts (usually according to 7/3 proportion), and one for training, the other for testing, which helps us to avoid <strong>over fitting</strong> (perform on training set well, but bad on new examples not in training set)</p>
<hr>
<h2 id="model-selection">Model Selection</h2>
<p>Take linear regression as a example, we use $h_{\theta}(x) = \theta_0 +\theta_1 x + \theta_2 x^2 +&hellip; $ as our hypothesis function, and we can add the degree of polynomial to make our hypothe/.sis better, but it may brings the over fitting problem, so we need to find out the best degree.</p>
<p>To  achieve so, we can compute the $J_{test}$(cost of different degrees $d$ on test set), and choose the best $d$, but it&rsquo;s only fit to the test set. So we divide the data set into three parts:</p>
<ul>
<li>training set (60% usually)</li>
<li>cross validation set (20% usually)</li>
<li>test set (20% usually)(check if the combo of $\theta$ and $\lambda$ has a good generalization of the problem, avoiding over fitting)</li>
</ul>
<p>and three kinds of cost function is $J_{train}$, $J_{cv}$, $J_{test}$.</p>
<p>Then for different $d(d=1, 2, 3, &hellip;)$, minimize $J(\theta)$ with $J_{train}$ and $J_{test}$, then compute the $J_{cv}$ for each $d$. We choose $d$ who has the lowest $J_{cv}$, and that.s how we do the model selection.</p>
<hr>
<h2 id="bias-vs-variance">Bias vs. Variance</h2>
<ul>
<li>
<img src="/article_imgs/bias_vs_var.JPG">
</li>
</ul>
<h3 id="diagnosing-bias-vs-variance">Diagnosing bias vs. variance</h3>
<ul>
<li>If $J_{train}$ is high and $J_{cv}\approx J_{train}$, we can tell it&rsquo;s a <strong>bias</strong> problem;</li>
<li>If $J_{train}$ is low and $J_{cv}\gg J_{train}$, we can tell it&rsquo;s a <strong>variance</strong> problem;</li>
</ul>
<hr>
<h2 id="about-regularization">About regularization</h2>
<p>As we know, appropriate $\lambda$ (regularization parameter) can help to prevent over fitting, but when the $\lambda$ is too large or too small. it won&rsquo;t work as so:</p>
<img src="/article_imgs/different_lambda.JPG">
<p>So choosing appropriate value of $\lambda$ is very necessary.</p>
<p>We can try different $\lambda$, minimize $J(\theta)$, then compute $J_{cv}$ (like what we do the model selection), and we can finf the best $\lambda$.</p>
<hr>
<h2 id="learning-curves">Learning Curves</h2>
<p>The learning curves describe the relationship of $m$(training set size) and error($J_{cv},J_{train}$), it&rsquo;s look like:</p>
<img src="/article_imgs/learningcurves.JPG" width="500">
<p>And if the algorithm is suffering from high bias, the learning curve is look like:</p>
<img src="/article_imgs/high_bias_curve.JPG" width="500">
<p>We can see from the graph that the increasing $m$ doesn&rsquo;t help to lower the bias, so we can conclude that getting more training data will not help to solve high-bias problem.</p>
<p>While if the algorithm is suffering from high variance, the learning curve is look like:</p>
<img src="/article_imgs/high_var_curve.JPG" width="500">
<p>From the graph we can see that there is a gap between $J_{cv}$ and $J_{train}$, and as $m$ increasing, the gap diminishes and the bias is also decreasing. So we can conclude that getting more training data is helpful to solve the high-variance problem.</p>
</div>


        
<div class="section bottom-menu">
    
<hr />
<p>


    
        <a href="/posts">back</a>
        
            &#183;
        
    

    
        
            <a href="/posts">Posts</a>
        
    
    
        
            &#183; 
            <a href="/scribble">Scribbles</a>
        
            &#183; 
            <a href="https://o3o.ca/@drapor">Feed</a>
        
            &#183; 
            <a href="/about">About Me</a>
        
    
    &#183; 
    <a href="https://drapor.me">
        
    </a>

</p>
</div>



        <div class="section footer">Copyright Â© 2020 Gabriel Drapor, All Rights Reserved.</div>


	
     <div id="gitalk-container"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script>
  var gittalk = new Gitalk({
      clientID: 'b33898060f51f3637cdb',
      clientSecret: '264aa9f9ab82c827765c14f5df4d8195bfd26a0d',
      repo: 'gabrieldrapor.github.io',
      owner: 'GabrielDrapor',
      admin: ['GabrielDrapor'],
      id: location.pathname,      
      distractionFreeMode: false  
  })
  gittalk.render("gitalk-container")
</script>
</body>

</html>
