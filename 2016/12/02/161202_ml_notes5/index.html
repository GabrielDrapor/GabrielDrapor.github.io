<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="robots" content="index, follow"><title>Machine Learning Notes 05 • DRAPORLAND</title><meta name="description" content="Machine Learning Notes 05 - Gabriel Drapor"><link rel="icon" href="/favicon.svg"><link rel="stylesheet" href="https://unpkg.com/nanoreset@3.0.1/nanoreset.min.css"><link rel="stylesheet" href="/css/theme.css"><link rel="search" type="application/opensearchdescription+xml" href="/atom.xml" title="DRAPORLAND"></head><body><div class="wrap" id="barba-wrapper"><header><h1 class="branding"><a href="/" title="DRAPORLAND"><img class="logo-image" src="/logo.svg" alt="logo"></a></h1><ul class="nav nav-list"><li class="nav-list-item"><a class="nav-list-link no-barba" href="/" target="_self">HOME</a></li><li class="nav-list-item"><a class="nav-list-link no-barba" href="/about-me" target="_self">ABOUT</a></li><li class="nav-list-item"><a class="nav-list-link no-barba" href="/2018-reading-list" target="_self">READING LIST</a></li><li class="nav-list-item"><a class="nav-list-link no-barba" href="/archives" target="_self">ARCHIVES</a></li><li class="nav-list-item"><a class="nav-list-link no-barba" href="/atom.xml" target="_self">RSS</a></li></ul></header><div class="barba-container"><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Machine Learning Notes 05</h1><div class="post-info"><a></a>2016-12-02</div><div class="post-content"><h2>Overfitting</h2>
<p>Overfitting would cause the modle to perform poorly, like this:<br>
<img src="http://7xugq7.com1.z0.glb.clouddn.com/reg_exmp1.PNG"></p>
<img src="http://7xugq7.com1.z0.glb.clouddn.com/reg_exmp2.PNG">
<p>  As above, plotting the hypothesis could be one way to try to decide what degree polynomial to use, but that doesn’t always work. Sometims we have too many features which cause that it’s difficult to visulize. And if we have a lot of features and very little training data, then overfitting can become a problem.</p>
<a id="more"></a>
<hr>
<h2>Two options to solve</h2>
<p><strong>1.Reduce the number of features</strong></p>
<blockquote>
<p>Manually select which features to keep or throw out<br>
Model selection algorithm (help ue to decide which features to keep or throw out automatically)</p>
</blockquote>
<blockquote>
<p>Disadvantages: Reduce some information about the problem.</p>
</blockquote>
<p><strong>2.Regularization</strong></p>
<blockquote>
<p>Keep all the features, but reduce magnitude/values of parameters <span>$\theta_{j}$</span><!-- Has MathJax -->.<br>
Works well when we have a lot of features, each of which contributes a bit to predicting <span>$y$</span><!-- Has MathJax -->.</p>
</blockquote>
<p>So we can see regularization is a better choice mostly.</p>
<hr>
<h2>See the cost functinon</h2>
<ul>
<li>First, consider this. If our hypothesis is like this:</li>
</ul>
<center><span>$$\theta_{0} + \theta_{1}x + \theta_{2}x^{2}
+ \theta_{3}x^3 + \theta_{4}x^4$$</span><!-- Has MathJax --></center>
<p>and if we penalize and make <span>$\theta_{3}$</span><!-- Has MathJax --> and <span>$\theta_{4}$</span><!-- Has MathJax --> really small, it means that <span>$\theta_{3}\approx0,\theta_{4}\approx0$</span><!-- Has MathJax -->, that is like as if we 're getting rid of these two terms, then we would find that</p>
<center><span>$\theta_{0} + \theta_{1}x + \theta_{2}x^{2} + \theta_{3}x^3 + \theta_{4}x^4\approx\theta_{0} + \theta_{1}x + \theta_{2}x^{2}$</span><!-- Has MathJax --></center>
<hr>
<h2>Regularization</h2>
<p>“…having <strong>smaller values of the parameters</strong> corresponds to usually <strong>smoother functions as well for the simpler</strong>. And which are therefore, also, <strong>less prone to overfitting</strong>. ”<br>
What we should do is to modify the cost function to shrink all of the parameters like this:</p>
<center><span>$J(\theta)=\frac{1}{2m}\sum_{i=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})^{2} + {\color{Blue} {\lambda\sum_{i=1}^{m}\theta_{j}^{2}}}$</span><!-- Has MathJax --></center>
<p>And the blue terms is called <strong>regularization terms</strong>, <span>$\lambda$</span><!-- Has MathJax --> is called <strong>regularization parameters</strong> which is used to trade off between two different goals:</p>
<span>$$\begin{cases}
1. \sum(h_{\theta}(x^{(i)})-y^{(i)})^{2}\:-\:fit\:well\\
2. \sum{\theta_{j}^{2}}\:-\:keep\:the\:parameters\:small
   \end{cases}$$</span><!-- Has MathJax -->
<p>By the way, we don’t penalize <span>$\theta_{0}$</span><!-- Has MathJax --> by convention.<br>
Besides, about the regularization parameters <span>$\lambda$</span><!-- Has MathJax -->, if <span>$\lambda$</span><!-- Has MathJax --> is too large, it would cause underfitting, and if <span>$\lambda$</span><!-- Has MathJax --> is too small, it may cause the useless regularization.</p>
<hr>
<h2>Regularized linear regression</h2>
<ul>
<li>Gradient descent:<br>
Old:<br>
   Repeat{<br>
      <span>$\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}\:(j=0,1,2,3,...,n)$</span><!-- Has MathJax --><br>
   }</li>
</ul>
<p><font color="blue">New:</font><br>
   <font color="blue">Repeat{</font><br>
      <span>${\color{Blue} {\theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum_{i=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})x_{0}^{(i)}}}$</span><!-- Has MathJax --><br>
      <span>${\color{Blue} {\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}\:(j=1,2,...,n)}}{\color{Red} {+\frac{\lambda}{m}\theta_{j}}}$</span><!-- Has MathJax --><br>
   <font color="blue">}</font></p>
<hr>
<h2>Regularization logistic regression</h2>
<p>   Cost function <span>$J(\theta)=-ylog(h_{\theta}(x))-(1-y)log(1-h_{\theta}(x)){\color{Blue} {+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}}}$</span><!-- Has MathJax --></p>
<p>   <font color="blue">Repeat{</font><br>
      <span>${\color{Blue} {\theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum_{i=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})x_{0}^{(i)}}}$</span><!-- Has MathJax --><br>
      <span>$\theta_{j}:=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}\:(j=1,2,...,n){\color{Red} {+\frac{\lambda}{m}\theta_{j}}}$</span><!-- Has MathJax --><br>
   <font color="blue">}</font><br>
Remember that <span>$h_{\theta}(x)=\frac{1}{1+e^{-\theta^{T}x}}$</span><!-- Has MathJax --></p>
</div></article></div></main><footer><div class="paginator"><a class="prev" href="/2016/12/02/161202_pyCrawler2/">prev</a><a class="next" href="/2016/12/01/161201_pyCrawler1/">next</a></div><div class="copyright"><p>&copy; 2015 - 2018 <a href="https://drapor.me">Drapor</a><br>Powered by <a href="https://hexo.io/" rel="noreferrer" target="_blank">Hexo</a></p></div></footer></div></div><script src="https://cdnjs.cloudflare.com/ajax/libs/barba.js/1.0.0/barba.min.js"></script><script>document.addEventListener('DOMContentLoaded', function() {
    Barba.Pjax.start()
})</script></body></html>